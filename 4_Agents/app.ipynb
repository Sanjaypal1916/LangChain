{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56be2dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612350a6",
   "metadata": {},
   "source": [
    "PRE-EXISTING API TOOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "75951dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=500)\n",
    "wiki = WikipediaQueryRun(api_wrapper=wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c784035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import ArxivQueryRun\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfab4dd",
   "metadata": {},
   "source": [
    "PRE-EXISTING API TOOLS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ac7aace",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_wrapper = ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=500)\n",
    "arxiv = ArxivQueryRun(api_wrapper=arxiv_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3d523c",
   "metadata": {},
   "source": [
    "CREATING NEW API TOOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89c6e8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71f1491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://python.langchain.com/docs/introduction/\")\n",
    "embedding = OllamaEmbeddings(model= \"llama2\")\n",
    "documents = loader.load()\n",
    "text_splitter  = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "texts = text_splitter.split_documents(documents[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4684f9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorstore = FAISS.from_documents(texts, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "103ddc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2f5e4408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "langchain_tool = create_retriever_tool(retriever,\"langsmith_search\",\n",
    "                      \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "44c9e8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mQuestion: What is LangChain?\n",
      "Thought: Let me check if there are any relevant documents on Arxiv or Wikipedia that can help me understand what LangChain is.\n",
      "Action: arxiv\n",
      "Action Input: \"LangChain\"\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mPublished: 2025-01-27\n",
      "Title: From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?\n",
      "Authors: Rodrigo Pedro, Daniel Castro, Paulo Carreira, Nuno Santos\n",
      "Summary: Large Language Models (LLMs) have found widespread applications in various\n",
      "domains, including web applications, where they facilitate human interaction\n",
      "via chatbots with natural language interfaces. Internally, aided by an\n",
      "LLM-integration middleware such as Langchain, user prompts are translated into\n",
      "SQL queries used by the LLM to provide meaningful responses to users. However,\n",
      "unsanitized user prompts can lead to SQL injection attacks, potentially\n",
      "compromising the security of the database. Despite the growing interest in\n",
      "prompt injection vulnerabilities targeting LLMs, the specific risks of\n",
      "generating SQL injection attacks through prompt injections have not been\n",
      "extensively studied. In this paper, we present a comprehensive examination of\n",
      "prompt-to-SQL (P$_2$SQL) injections targeting web applications based on the\n",
      "Langchain framework. Using Langchain as our case study, we characterize\n",
      "P$_2$SQL injections, exploring their variants and impact on application\n",
      "security through multiple concrete examples. Furthermore, we evaluate 7\n",
      "state-of-the-art LLMs, demonstrating the pervasiveness of P$_2$SQL attacks\n",
      "across language models. Our findings indicate that LLM-integrated applications\n",
      "based on Langchain are highly susceptible to P$_2$SQL injection attacks,\n",
      "warranting the adoption of robust defenses. To counter these attacks, we\n",
      "propose four effective defense techniques that can be integrated as extensions\n",
      "to the Langchain framework. We validate the defenses through an experimental\n",
      "evaluation with a real-world use case application.\n",
      "\n",
      "Published: 2023-10-09\n",
      "Title: Automating Customer Service using LangChain: Building custom open-source GPT Chatbot for organizations\n",
      "Authors: Keivalya Pandya, Mehfuza Holia\n",
      "Summary: In the digital age, the dynamics of customer service are evolving, driven by\n",
      "technological advancements and the integration of Large Language Models (LLMs).\n",
      "This research paper introduces a groundbreaking approach to automating customer\n",
      "service using LangChain, a custom LLM tailored for organizations. The paper\n",
      "explores the obsolescence of traditional customer support techniques,\n",
      "particularly Frequently Asked Questions (FAQs), and proposes a paradigm shift\n",
      "towards responsive, context-aware, and personalized customer interactions. The\n",
      "heart of this innovation lies in the fusion of open-source methodologies, web\n",
      "scraping, fine-tuning, and the seamless integration of LangChain into customer\n",
      "service platforms. This open-source state-of-the-art framework, presented as\n",
      "\"Sahaay,\" demonstrates the ability to scale across industries and\n",
      "organizations, offering real-time support and query resolution. Key elements of\n",
      "this research encompass data collection via web scraping, the role of\n",
      "embeddings, the utilization of Google's Flan T5 XXL, Base and Small language\n",
      "models for knowledge retrieval, and the integration of the chatbot into\n",
      "customer service platforms. The results section provides insights into their\n",
      "performance and use cases, here particularly within an educational institution.\n",
      "This research heralds a new era in customer service, where technology is\n",
      "harnessed to create efficient, personalized, and responsive interactions.\n",
      "Sahaay, powered by LangChain, redefines the customer-company relationship,\n",
      "elevating customer retention, value extraction, and brand image. As\n",
      "organizations embrace LLMs, customer service becomes a dynamic and\n",
      "customer-centric ecosystem.\n",
      "\n",
      "Published: 2024-06-26\n",
      "Title: Poisoned LangChain: Jailbreak LLMs by LangChain\n",
      "Authors: Ziqiu Wang, Jun Liu, Shengkai Zhang, Yang Yang\n",
      "Summary: With the development of natural language processing (NLP), large language\n",
      "models (LLMs) are becoming increasingly popular. LLMs are integrating more into\n",
      "everyday life, raising public concerns about their security vulnerabilities.\n",
      "Conseq\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: What is LangChain?\n",
      "\n",
      "Thought: I should check if there are any relevant documents on Arxiv or Wikipedia that can help me understand what LangChain is.\n",
      "\n",
      "Action: arxiv\n",
      "Action Input: \"LangChain\"\n",
      "\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mNo good Arxiv Result was found\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: What is LangChain?\n",
      "\n",
      "Thought: I should check if there are any relevant documents on Wikipedia that can help me understand what LangChain is.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: \"LangChain\"\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mPage: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
      "\n",
      "Page: Robert Lang Studios\n",
      "Summary: Robert Lang Studios is a recording studio in Shoreline, Washington, United States. Numerous bands have recorded at Robert Lang Studios since 1974 including Nirvana, Alice in Chains, Foo Fighters, Soundgarden, Dave Matthews Band, Death Cab for Cutie, Heart, Sir Mix-A-Lot, Peter Frampton, Candlebox, and Bush.\n",
      "\n",
      "Page: Cosmo Gordon Lang\n",
      "Summary: William Cosmo Gordon Lang, 1st Baron Lang of Lambeth,  (31 October 1864 – 5 December 1945) was a Scottish Anglican prelate who served as Archbishop of York (1908–1928) and Archbishop of Canterbury (1928–1942). His elevation to Archbishop of York, within 18 years of his ordination, was the most rapid in modern Church of England history. As Archbishop of Canterbury during the abdication crisis of 1936, he took a strong moral stance, his comments in a subsequent broadcast being widely condemned as uncharitable towards the departed king.\n",
      "The son of a Scots Presbyterian minister, Lang abandoned the prospect of a legal and political career to train for the Anglican priesthood. Beginning in 1890, his early ministry was served in slum parishes in Leeds and Portsmouth, except for brief service as Vicar of the University Church of St Mary the Virgin in Oxford. In 1901 he was appointed suffragan Bishop of Stepney in London, where he continued his work among the poor. He also served as a canon of St Paul's Cathedral, London.\n",
      "In 1908 Lang was nominated as Archbishop of York, despite his relatively junior status as a suffragan rather than a diocesan bishop. His religious stance was broadly Anglo-Catholic, tempered by the liberal Anglo-Catholicism advocated in the Lux Mundi essays. He consequently entered the House of Lords as a Lord Spiritual and caused consternation in traditionalist circles by speaking and voting against the Lords' proposal to reject David Lloyd George's 1909 \"People's Budget\". This radicalism was not maintained in subsequent years. At the start of the First World War, Lang was heavily criticised for a speech in which he spoke sympathetically of the German Emperor. This troubled him greatly and may have contributed to the rapid ageing which affected his appearance during the war years. After the war he began to promote church unity and at the 1920 Lambeth Conference was responsible for the Church's Appeal to All Christian People. As Archbishop of York he supported controversial proposals for the 1928 revision of the Book of Common Prayer but, after acceding to Canterbury, he took no practical steps to resolve this issue.\n",
      "Lang became Archbishop of Canterbury in 1928. He presided over the 1930 Lambeth Conference, which gave limited church approval to the use of contraception. After denouncing the Italian invasion of Abyssinia in 1935 and strongly condemning European antisemitism, Lang later supported the appeasement policies of the British government. In May 1937 he presided over the coronation of King George VI and Queen Elizabeth. On retirement in 1942 Lang was raised to the peerage as Baron Lang of Lambeth and continued to attend and speak in House of Lords debates until his death in 1945. Lang himself believed that he had not lived up to his own high standards. Others have praised his qualities of industry, his efficiency and his commitment to his calling.\n",
      "\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent stopped due to iteration limit or time limit.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "\n",
    "wikipedia = WikipediaAPIWrapper()\n",
    "arxivapi = ArxivAPIWrapper()\n",
    "# Create dummy tools\n",
    "wiki = WikipediaQueryRun(api_wrapper=wikipedia)\n",
    "arxiv = ArxivQueryRun(api_wrapper=arxivapi)\n",
    "\n",
    "\n",
    "retriever_tool = Tool(\n",
    "    name=\"DocumentRetriever\",\n",
    "    func=lambda q: retriever.get_relevant_documents(q),\n",
    "    description=\"Use this to fetch documents related to a user query.\"\n",
    ")\n",
    "\n",
    "tools = [retriever_tool, wiki, arxiv]\n",
    "\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    ")\n",
    "\n",
    "response = agent.run(\"What is LangChain?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "71a29f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This output parser only works on ChatGeneration output",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     15\u001b[39m agent = create_openai_functions_agent(\n\u001b[32m     16\u001b[39m     llm=llm,\n\u001b[32m     17\u001b[39m     tools=tools,\n\u001b[32m     18\u001b[39m     prompt=prompt,\n\u001b[32m     19\u001b[39m )\n\u001b[32m     21\u001b[39m agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m response = \u001b[43magent_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is LangChain?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sanjay\\Desktop\\AI ML\\DEV\\Langchain\\venv\\Lib\\site-packages\\langchain\\chains\\base.py:170\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    169\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    171\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sanjay\\Desktop\\AI ML\\DEV\\Langchain\\venv\\Lib\\site-packages\\langchain\\chains\\base.py:160\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    159\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    162\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    163\u001b[39m     )\n\u001b[32m    165\u001b[39m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    166\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    167\u001b[39m     )\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sanjay\\Desktop\\AI ML\\DEV\\Langchain\\venv\\Lib\\site-packages\\langchain\\agents\\agent.py:1624\u001b[39m, in \u001b[36mAgentExecutor._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m   1622\u001b[39m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[32m   1623\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_continue(iterations, time_elapsed):\n\u001b[32m-> \u001b[39m\u001b[32m1624\u001b[39m     next_step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1627\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1630\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1631\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[32m   1632\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._return(\n\u001b[32m   1633\u001b[39m             next_step_output, intermediate_steps, run_manager=run_manager\n\u001b[32m   1634\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sanjay\\Desktop\\AI ML\\DEV\\Langchain\\venv\\Lib\\site-packages\\langchain\\agents\\agent.py:1330\u001b[39m, in \u001b[36mAgentExecutor._take_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1321\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_take_next_step\u001b[39m(\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1323\u001b[39m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1327\u001b[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1328\u001b[39m ) -> Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[32m   1329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._consume_next_step(\n\u001b[32m-> \u001b[39m\u001b[32m1330\u001b[39m         \u001b[43m[\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1338\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1340\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sanjay\\Desktop\\AI ML\\DEV\\Langchain\\venv\\Lib\\site-packages\\langchain\\agents\\agent.py:1358\u001b[39m, in \u001b[36mAgentExecutor._iter_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1355\u001b[39m     intermediate_steps = \u001b[38;5;28mself\u001b[39m._prepare_intermediate_steps(intermediate_steps)\n\u001b[32m   1357\u001b[39m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1358\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_action_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1364\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.handle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sanjay\\Desktop\\AI ML\\DEV\\Langchain\\venv\\Lib\\site-packages\\langchain\\agents\\agent.py:465\u001b[39m, in \u001b[36mRunnableAgent.plan\u001b[39m\u001b[34m(self, intermediate_steps, callbacks, **kwargs)\u001b[39m\n\u001b[32m    457\u001b[39m final_output: Any = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream_runnable:\n\u001b[32m    459\u001b[39m     \u001b[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001b[39;00m\n\u001b[32m    460\u001b[39m     \u001b[38;5;66;03m# streaming\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    463\u001b[39m     \u001b[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n\u001b[32m    464\u001b[39m     \u001b[38;5;66;03m# accumulate the output into final output and return that.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunnable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfinal_output\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sanjay\\Desktop\\AI ML\\DEV\\Langchain\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3440\u001b[39m, in \u001b[36mRunnableSequence.stream\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3433\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   3434\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstream\u001b[39m(\n\u001b[32m   3435\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3438\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   3439\u001b[39m ) -> Iterator[Output]:\n\u001b[32m-> \u001b[39m\u001b[32m3440\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sanjay\\Desktop\\AI ML\\DEV\\Langchain\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3426\u001b[39m, in \u001b[36mRunnableSequence.transform\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3419\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   3420\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtransform\u001b[39m(\n\u001b[32m   3421\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3424\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   3425\u001b[39m ) -> Iterator[Output]:\n\u001b[32m-> \u001b[39m\u001b[32m3426\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transform_stream_with_config(\n\u001b[32m   3427\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3428\u001b[39m         \u001b[38;5;28mself\u001b[39m._transform,\n\u001b[32m   3429\u001b[39m         patch_config(config, run_name=(config \u001b[38;5;129;01mor\u001b[39;00m {}).get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.name),\n\u001b[32m   3430\u001b[39m         **kwargs,\n\u001b[32m   3431\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sanjay\\Desktop\\AI ML\\DEV\\Langchain\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2211\u001b[39m, in \u001b[36mRunnable._transform_stream_with_config\u001b[39m\u001b[34m(self, input, transformer, config, run_type, **kwargs)\u001b[39m\n\u001b[32m   2209\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2210\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2211\u001b[39m         chunk: Output = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   2212\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[32m   2213\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sanjay\\Desktop\\AI ML\\DEV\\Langchain\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3388\u001b[39m, in \u001b[36mRunnableSequence._transform\u001b[39m\u001b[34m(self, input, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   3385\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3386\u001b[39m         final_pipeline = step.transform(final_pipeline, config)\n\u001b[32m-> \u001b[39m\u001b[32m3388\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m final_pipeline\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sanjay\\Desktop\\AI ML\\DEV\\Langchain\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1436\u001b[39m, in \u001b[36mRunnable.transform\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   1433\u001b[39m             final = ichunk\n\u001b[32m   1435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[32m-> \u001b[39m\u001b[32m1436\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream(final, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sanjay\\Desktop\\AI ML\\DEV\\Langchain\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1003\u001b[39m, in \u001b[36mRunnable.stream\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    985\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstream\u001b[39m(\n\u001b[32m    986\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    987\u001b[39m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[32m    988\u001b[39m     config: Optional[RunnableConfig] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    989\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m    990\u001b[39m ) -> Iterator[Output]:\n\u001b[32m    991\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Default implementation of stream, which calls invoke.\u001b[39;00m\n\u001b[32m    992\u001b[39m \n\u001b[32m    993\u001b[39m \u001b[33;03m    Subclasses should override this method if they support streaming output.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1001\u001b[39m \u001b[33;03m        The output of the Runnable.\u001b[39;00m\n\u001b[32m   1002\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1003\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sanjay\\Desktop\\AI ML\\DEV\\Langchain\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:204\u001b[39m, in \u001b[36mBaseOutputParser.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    197\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result(\n\u001b[32m    198\u001b[39m             [ChatGeneration(message=inner_input)]\n\u001b[32m   (...)\u001b[39m\u001b[32m    202\u001b[39m         run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    203\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sanjay\\Desktop\\AI ML\\DEV\\Langchain\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1933\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1929\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1930\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1931\u001b[39m         output = cast(\n\u001b[32m   1932\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1933\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1935\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1936\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1937\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1938\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1939\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1941\u001b[39m         )\n\u001b[32m   1942\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1943\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sanjay\\Desktop\\AI ML\\DEV\\Langchain\\venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sanjay\\Desktop\\AI ML\\DEV\\Langchain\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:205\u001b[39m, in \u001b[36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[39m\u001b[34m(inner_input)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    197\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result(\n\u001b[32m    198\u001b[39m             [ChatGeneration(message=inner_input)]\n\u001b[32m   (...)\u001b[39m\u001b[32m    202\u001b[39m         run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    203\u001b[39m     )\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    206\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    207\u001b[39m     config,\n\u001b[32m    208\u001b[39m     run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    209\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sanjay\\Desktop\\AI ML\\DEV\\Langchain\\venv\\Lib\\site-packages\\langchain\\agents\\output_parsers\\openai_functions.py:83\u001b[39m, in \u001b[36mOpenAIFunctionsAgentOutputParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_result\u001b[39m(\n\u001b[32m     80\u001b[39m     \u001b[38;5;28mself\u001b[39m, result: List[Generation], *, partial: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     81\u001b[39m ) -> Union[AgentAction, AgentFinish]:\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result[\u001b[32m0\u001b[39m], ChatGeneration):\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThis output parser only works on ChatGeneration output\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     84\u001b[39m     message = result[\u001b[32m0\u001b[39m].message\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._parse_ai_message(message)\n",
      "\u001b[31mValueError\u001b[39m: This output parser only works on ChatGeneration output"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_openai_functions_agent, AgentExecutor\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "from langchain import hub\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever=retriever,\n",
    "    name=\"document_retriever\",\n",
    "    description=\"Retrieves documents based on user queries.\"\n",
    ")\n",
    "\n",
    "tools = [retriever_tool, wiki, arxiv]\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "agent = create_openai_functions_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "response = agent_executor.invoke({\"input\": \"What is LangChain?\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6655e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
