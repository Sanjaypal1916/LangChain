{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64c0e59e-a19e-4a31-b773-f76ac45b3b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../secrets')\n",
    "from secret_keys import hugging_face\n",
    "import os \n",
    "os.environ[\"Huggingface_API\"] = hugging_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "517fcd1a-c7a1-4a19-b736-da0b12169abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cea2bd98-6d67-46c9-bce3-d49231f5a25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "C:\\Users\\sanjay\\anaconda3\\envs\\py310\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\sanjay\\anaconda3\\envs\\py310\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to technology and innovation.\n",
      "\n",
      "1. ChainInnovate.com\n",
      "2. BlockTechPioneers.com\n",
      "3. NFTFrontier.com\n",
      "4. TechNFT.com\n",
      "5. BlockchainInnovators.net\n",
      "6. NFTTrendsetters.com\n",
      "7. CryptoVanguard.com\n",
      "8. TechNFTInnovations.com\n",
      "9. BlockchainPioneers.net\n",
      "10. NFTTechPioneers.com\n",
      "11. InnovativeBlockchain.net\n",
      "12. NFTInnovators.com\n",
      "13. BlockchainTrendsetters.com\n",
      "14. TechNFTInnovations.net\n",
      "15. NFTBlockchainPioneers.com\n",
      "16. InnovativeNFT.net\n",
      "17. BlockchainTechTrends.com\n",
      "18. NFTInnovation.net\n",
      "19. TechNFTPioneers.com\n",
      "20. BlockchainInnovation.net\n",
      "\n",
      "These names are designed to be authentic and technology-focused, while also incorporating the themes of blockchain and NFTs. The use of words like \"innovate,\" \"pioneers,\" \"trendsetters,\" and \"vanguard\" emphasize the cutting-edge nature of the technology, while \"tech\" and \"blockchain\" make it clear that the site is related to the field. The inclusion of \"NFT\" ensures that visitors understand that the site is also focused on non-fungible tokens.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceEndpoint\n",
    "\n",
    "# Paste your actual token here\n",
    "HUGGINGFACEHUB_API_TOKEN = \"your_hf_token_here\"\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    task=\"text-generation\",\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    temperature=0.7,\n",
    "    max_length=128,\n",
    "    huggingfacehub_api_token=hugging_face\n",
    ")\n",
    "\n",
    "response = llm(\"Suggest some names for my website related to blockchain and NFTs.it should authentic and should be related\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1900def-fdfc-4f13-9716-c3818cfc416c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! trust_remote_code is not default parameter.\n",
      "                    trust_remote_code was transferred to model_kwargs.\n",
      "                    Please make sure that trust_remote_code is what you intended.\n",
      "C:\\Users\\sanjay\\anaconda3\\envs\\py310\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to my website\n",
      "Here are some names related to blockchain and NFTs that may suit your website:\n",
      "1. CryptoBlockchain\n",
      "2. NFT Galaxy\n",
      "3. BlockchainBazaar\n",
      "4. NFTify\n",
      "5. CryptoCoinCrypto\n",
      "6. NFT Marketplace\n",
      "7. CryptoBlockchain.com\n",
      "8. NFT Collectors Club\n",
      "9. NFT Art World\n",
      "10. NFT Revolution\n"
     ]
    }
   ],
   "source": [
    "TUI = HuggingFaceEndpoint(\n",
    "    task=\"text-generation\",\n",
    "    repo_id=\"tiiuae/falcon-7b-instruct\",\n",
    "    # max_new_tokens= 100,  # must be â‰¤ 250\n",
    "    temperature= 0.7,\n",
    "    trust_remote_code= True,\n",
    "    huggingfacehub_api_token=hugging_face\n",
    ")\n",
    "\n",
    "response = TUI(\"Suggest only 10 names for my website related to blockchain and NFTs.it should authentic and should be related\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92ac6c5c-67f1-4469-92e1-04e1b3895560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanjay\\anaconda3\\envs\\py310\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' on\\n\\nThe model I am using is trained on data from 1950 to 2019. It is a time series model for forecasting, and it is trained on historical data of various economic indicators such as GDP, inflation, unemployment rate, etc. The model is designed to predict these indicators for future years, and it is updated regularly with new data.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"uptill which year your model is trained\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666fbc9f-0528-472a-81a5-bf3c1cb3fc77",
   "metadata": {},
   "source": [
    "HUggingFacePipeLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "074464a3-2d31-4dde-80b3-41934da2de21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3de44d7b-f0f9-4092-accc-56d068a4efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "que = \"50 words for elon musk\"\n",
    "template = \"\"\"\n",
    "Question :    {que}\n",
    "Answer   :    lets think about it step-by-step\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"que\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60d1bfdf-47fd-400d-a400-98715bba624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = LLMChain(llm =llm, prompt = prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7be4b663-2896-4810-b4c1-6ede9e6aa467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanjay\\anaconda3\\envs\\py310\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'que': '50 words for elon musk',\n",
       " 'text': \"\\n1. Entrepreneur: Co-founded PayPal, SpaceX, Tesla, Neuralink, and The Boring Company.\\n2. Innovator: Pioneering electric cars, space travel, renewable energy, and neural lace technology.\\n3. Visionary: Believes in colonizing Mars and making humanity a multi-planetary species.\\n4. Philanthropist: Committed to solving global issues like climate change, AI safety, and poverty.\\n5. Pioneer: Driving advancements in renewable energy with SolarCity and Tesla.\\n6. Aerospace Engineer: Revolutionizing space travel with SpaceX and reusable rockets.\\n7. Space Explorer: Aiming to send humans to Mars and establish a self-sustaining city.\\n8. Futurist: Believes in using technology to create a better future for humanity.\\n9. Disruptor: Challenging the status quo in various industries, including transportation and energy.\\n10. Technopreneur: Combining technology and entrepreneurship to create groundbreaking companies.\\n11. Visionary Leader: Inspiring millions to dream big and push the boundaries of what's possible.\\n12. Risk-Taker: Willing to take bold risks in pursuit of his ambitious goals.\\n13. Transformative: Transforming the automotive, aerospace, and energy industries.\\n14. Trailblazer: Paving the way for a more sustainable and technologically advanced future.\\n15. Game-Changer: Disrupting traditional industries with innovative solutions.\\n16. Dreamer: Believes in dreaming big and making those dreams a reality.\\n17. Renaissance Man: Expert in various fields, including physics, computer science, and engineering.\\n18. Visionary Engineer: Bridging the gap between science fiction and reality.\\n19. Technological Pioneer: Advancing technology to solve some of humanity's most pressing challenges.\\n20. Pioneer of the New Space Age: Leading the way in space exploration and colonization.\\n21. Futurist Entrepreneur: Combining futurism and entrepreneurship to create a better future.\\n22. Transformational Leader: Driving change and innovation in various industries.\\n23. Innovative Thinker: Const\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLM.invoke(que)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93021475-3a19-4b7d-b5d6-c677586017e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76c06613-2c72-4a15-a117-1067e217536b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f9ca65aa234f78a4dd249ad4dcaa69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  88%|########8 | 482M/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanjay\\anaconda3\\envs\\py310\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sanjay\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8288b7f12cad43d5b8e5b5cd0d4a13d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5eb42a59944eff96e8ccc8aabae7d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75681dad39b47898e5dcac4ac40c1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fda0ecf4e084de4acbcb2b0f26f7fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18159ab56aa645a2b5e9562316e663c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"gpt2\"\n",
    "model= AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer =AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe0ef46a-d8f8-4d5d-8c73-112213ecd14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f571617d-29b3-426c-895a-f07bb957d3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3f73ff0-52e9-4bad-a25c-bd14be1ec95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33fed051-28ae-442a-8979-c1cd3d4685d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory growth set for GPU.\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Memory growth set for GPU.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cac4e4cd-b9e0-41d2-be48-0e698492049f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(task = \"text-generation\", model = model, tokenizer=tokenizer, max_new_tokens = 5, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd0d7bc1-4dc5-41b1-97ae-41cd785f7f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da6ae8c7-8c2e-4f26-9313-f871a584d71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'give 10 authentic names for my website in this post. Click'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf.invoke(\"give 10 authentic names for my website\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93be1968-a83d-4fec-93eb-20b226d36c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243a3756-f829-4a1e-aa1c-ce0cf5738c14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
